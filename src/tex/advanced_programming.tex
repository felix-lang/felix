\documentclass[oneside]{book}
\usepackage{xcolor}
\definecolor{bg}{rgb}{0.95,0.95,0.95}
\definecolor{exbg}{rgb}{0.80,0.95,0.95}
\definecolor{emphcolor}{rgb}{0.5,0.0,0.0}
\newcommand{\empha}{\bf\color{emphcolor}}
\usepackage[framemethod=TikZ]{mdframed}
\newtheorem{example}{Example}
\mdfdefinestyle{MyFrame}{innerleftmargin=20pt}
\newenvironment{myexample}%
  {\begin{mdframed}[style=MyFrame,backgroundcolor=exbg]}%
  {\end{mdframed}}
\usepackage{parskip}
\usepackage{minted}
\usepackage{caption}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amscd}
\usemintedstyle{friendly}
\setminted{bgcolor=bg,xleftmargin=20pt}
\usepackage{hyperref}
\hypersetup{pdftex,colorlinks=true,allcolors=blue}
\usepackage{hypcap}
\title{Advanced Programming Technniques}
\author{John Skaller}
\begin{document}
\maketitle
\tableofcontents
\chapter{Review}
Here we are going to review some basic concepts you should already know.

\section{Invariants}
An {\em invariant} as you might expect is something which does not change.

\subsection{Loop Invariants}

Consider this code snippet:
\begin{minted}{c++}
int main() {
  for(int i=0; i<100; ++i) {
    int j  = 1;
    int k = j + i;
    cout << k << endl; 
  }
}
\end{minted}

Here the variable j has the same value 1, no matter which iteration
of the loop is being executed. So we can say j is a loop invariant.
There is an optimisation known as {\em invariant code motion} which
allows us to rewrite the code to be more efficient without changing
the semantics:
\begin{minted}{c++}
int main() {
  int j  = 1;
  for(int i=0; i<100; ++i) {
    int k = j + i;
    cout << k << endl; 
  }
}
\end{minted}
So an advantage of recognising invariants is that doing so
can lead to better performance by {\em optimisation}.

In this case we can make a further modification:
\begin{minted}{c++}
  int const j = 1;
\end{minted}
which probably has no impact on performance of the code but
has a significant impact on ability to reason about the code.
We are using a language feature which assures us that 
the variable \verb%j% is indeed invariant.

Notice we could have done this in the first program too.
In that case, the \verb%const% qualification would have assured us
only that the variable did not change during a single iteration
of the loop, whereas, due to scoping rules, the modification
tells us it does not change in the \verb%main% function.

\subsection{Public Invariants}
Public invariants or {\em semantic laws}, are rules which 
define properties of one or more types in the abstract.

{\em Abstract} has a technical meaning which is not well understood
so we need to spell it out: abstract means that the semantics
are defined in terms of the interactions of functions.

In particular an {\em abstract data type} or ADT is defined
by a collection of functions and types. This notion of abstraction
is compounded with a notion of functional composition to provide
a {\em category theoretic model of computing}.

\subsubsection{Group}
Let us consider a single type as follows:

\begin{minted}{c++}
class G {
public:
  G operator +(G) const;
  G operator -() const;
  static G Zero()const;
}
\end{minted}

The following laws are written as executable code on values of G,
with an assumption with have an equality operator:

\begin{minted}{c++}
  // unit law
  assert (Zero() + g == g);     

  // associatibity
  assert (g1 + (g2 + g3) == (g1 + g2) + g3); 

  // inverse
  assert (g + -g == Zero());  
\end{minted}

A type which obeys these laws is called a {\em group}. If the following
law is also obeyed:

\begin{minted}{c++}
  // symmetry
  assert (g1 + g2 = g2 + g1);
\end{minted}

then it is called a symmetric or Abelian group. When the operator
is given as plus, it is called an additive group.  Multiplicative
groups use a multiply operator and One() as the unit and the inverse
would be written as a method named \verb%reciprocal%.

You will probably
recognise that, within the limitation of size, \verb%int% is an
additive group but it is not multiplicative group. 

You may think, floating point numbers form a multiplicative group
if you take out zero. This is not the case! Floating point operations
are not associative, so in fact floats do not even form an additive group!


\subsubsection{Equivalence relation}
This is a fundamental abstraction that must be fully understood
because it defines what equality means.

\begin{minted}{c++}
  // reflexivity
  assert (v == v);

  // symmetry 
  assert ((v1 == v2) ? v2 == v1) : true);

  // transitivity
  assert (v1 == v2 && v2 == v3 ? v1 == v3 : true);
\end{minted}

The C conditional operator is used here so we can write the law
as an expression which can be asserted.


\subsubsection{Stack}
A more computer science based example is the definition of a stack.

\begin{minted}{c++}
   stack s ...
   ssave = s;
   s.push(v);
   vnew = stack.pop();
   assert (v == vnew);
   assert (s == ssave);
\end{minted}

This says if you push something onto a stack then do a pop, you get
the last thing you pushed back, and, the stack is in the same state
as before the push.  These assertions apply to any type S like:

\begin{minted}{c++}
class S {
public:
  void push(V);
  V pop();
};
\end{minted}

noting again the requirement we have an equality operator.

\subsection{Representation Invariants}
Another popular example of the utility of the concept of an invariant
is shown here:
\begin{minted}{c++}
class rational {
  int numerator;
  int denominator;
public:
   rational(int num, int den) {
     if(den == 0) throw "zero denominator";
     int sign = sgn(num) * sgn(den);
     unsigned int uden = abs(den);
     unsigned int unum = abs(num);
     unsigned int g = gcd(uden, unum);
     numerator = sign * unum / g;
     denominator = uden / g;
   }
   ...
};
\end{minted}
Here the constructor is dynamically enforcing an invariant that
the denominator of the representation is a positive integer
which is relatively prime with respect to the numberator.
This ensures the denominator cannot be zero, and if the rational
number is negative the sign will be found in the numerator.

The division by the greatest common divisor of the the input
values ensures the stored representation is minimal, that is,
the values are as close to zero as possible. This increases the
set of rational numbers available to the maximium possible
with the given representation.

Now we will write a method to multiple the value by another rational
number:

\begin{minted}{c++}
  operator *= (rational other) { 
    int num = numerator * other.numerator;
    int den = denominator * other.denomintaor;
    int g = gcd(num,den);
    numerator = num / g;
    denominator = den / g;
  }
\end{minted}
First we need to check that the representation invariant is maintained.
Clearly we have code there to ensure the relatively prime part is obeyed.
But now we see the advantage of an invariant in two ways: we obtain
both optimisation of the code {\em and} ease of reasoning as a result:
the new denominator must be positive because the product of two positive
values is positive, and because we assume the gcd of two integers is always
positive.

The reasoning is easy and the code is shorter than in the constructor
so not only is our multiply likely to be correct, the resulting representation
maintains the invariant.

It may seem this is a perfect piece of code but alas it is a not,
it is a very bad piece of code as we shall see in the next section!

Here is why our code is bad:
\begin{minted}{c++}
int main(){
   rational x(5,7);
   void mulby(thread.id *pid, int n, int d) { 
     pid = thread.get_id(); 
     x *= rationaln,d);
   }
   thread.id id1;
   thread.id id2;
   thread (f, &id1, 42, 7);
   thread (f, &id2, 16, 8);
   thread.join(id1);
   thread.join(id2);
   cout << x << endl;
}
\end{minted}

We spawn two threads, each of which multiplies x by a rational number,
join the threads with the main thread and print the result.

Unfortunately, the multiplication method is not thread safe, and there
can be several {\em race} between the two multiplications. First,
both threads could fetch the old value at the same time, do the multiplies
in some order and store the results in order, but then the result would
be the orginal value muliplied by one or other of the new values when
we wanted it to be multiplied by both.

But worse is possible! When storing the new representation the first thread
might store the numberator, then the second stores the numerator and
denominator then the first finally stores the denominator. The result is not
only wrong, there's no assurance the relatively prime invariant is maintained.

We can fix this by adding a lock to the representation and
locking it at the start of the multiplication method, releasing it
when finished. But then we are paying a price: we're using extra
space and incurring a performance hit, even if our application
is single threaded.

Instead, let us try to use inheritance to solve the problem:
\begin{minted}{c++}
class rational { ..
protected:
  void unsafe_mulby(rational other) {
    int num = numerator * other.numerator;
    int den = denominator * other.denomintaor;
    int g = gcd(num,den);
    numerator = num / g;
    denominator = den / g;
  }
private:
  virtual void mulby(rational other) { unsafe_mulby(other); }
public:
  void operator *= (rational other) { mulby(other); }
};
class ts_rational : public rational {
  atomic<bool> lock;
public:
  ts_rational(int n, int d) : rational (n,d), lock(false) {}
private:
  override void mulby (rational other) {
    while(lock.exchange(true, memory_order_acquire)); // lock
    unsafe_mulby(other);
    lock.store(false, memory_order_release); // unlock
  }
}; 
\end{minted}

This code is a lot more complicated! The core multiplication is
put in the \verb%unsafe_mulby% routine. This method is protected, so it
cannot be called by the public, but it can be called in a derived class.

We then define a virtual method \verb%mulby% which is entirely private,
so it can only be called in the \verb%rational% class, and then we 
define the public \verb%operator *=% method to call it.

Finally in the derived class, we need only define the overriding
\verb%mulby% method which calls the \verb%unsafe_mulby% method
to do the work inside a critical section provided by the spinlock.

The structure presented here is the {\em only} correct way to organise
this. Note that contrary to popular myths propagated by some very
well known authors, virtual functions in C++ should always be private!
Furthermore, the basic public method of the base class should generally
be wrappers dispatching to these virtuals. In pathicular overriding
virtuals should never be called explicitly, and should actually be
entirely hidden, only C++ provides no hidden access modifier.
The only way the override should ever be invoked is by a virtual
dispatch from the call in the base.

Our code indicates an important principle: public constructors must
establish public invariants, and public methods must maintain them.
However,  public methods should only be called by the public!
Never call public methods from any methods in your class!

The reason for this rule is simple enough: public methods
must do extra work establishing and maintaining representation
invariants. In addition, the body of a public method may often
be augmented with debugging code to trace the history of public
access to the class, and we don't went internal calls to mess
up the tracing.

In fact, non-public methods need not maintain public invariants:
public invariants are pre- and post-conditions on public methods.
Once the pre-condition is established an inner call in a public
method can assume the pre-condition rather than checking it.
The post condition of an inner call need not obey any rules at
all, because more work may be done.

So, all in all, we appear to have solved the problem conditionally:
we have imposed a responsibility on the programmer to use the
\verb%rational% class if the application is single threaded,
or use the \verb%ts_rational% class if multi-threaded.

Also the code is more complex and harder to reason about.

Don't believe me? Well you'd better because the code is {\em still wrong!}

The problem is simple enough: there is an operation we are using
implicitly which is provided by the compiler and the compiler
implementation is not thread safe. Note we pass the argument to
the multiplication method by value which might invoke the default
copy constructor. This will copy the non-static data members
\verb%num% and \verb%den% one after the other with a pre-emption
possible in between so the values might be changed by another thread
half way through. We have another data race!

How can we fix this?

You are going to hate the answer. It cannot be fixed!
If you are a fan of object orientation it is about time
you woke up. Object Orientation is a false paradigm.
It must be abandoned because quite simply it does not work.
This is not to say that classes and virtual dispatch are not
useful! They certainly are, but they do not solve all problems.

In particular any problem in which functions have two variant
arguments cannot have an OO solution. This has been known for
decades and is the problem is called the {\em covariance problem}.

Let me elaborate the issue with our example. The default
copy constructor for \verb%rational% is perfectly good in a single
threaded context. However to maintain the representation invariants
in a multithreaded context, not to mention actually getting the 
right abstract value, it must do the copying atomically.

To do this a lock is required and we have one! The problem is it
is in the wrong place! It is in the object, not the argument,
and the argument type \verb%rational% makes no provision for a lock!

So we can fix that with a copy constructor for \verb%ts_rational%,
right? Certainly we can do that:

\begin{minted}{c++}
  ts_rational(ts_rational const& other) { 
    while(lock.exchange(true, memory_order_acquire)); // lock
    numerator = other.numerator;
    denonminator = other.denominator; 
    lock.store(false, memory_order_release); // unlock
  }
\end{minted}

The problem is, this constructor is not called passing a \verb%rational%
argument, only a \verb%ts_rational% argument so lets change the argument
types to \verb%ts_rational%, ok?

Woops! Its not ok at all because now \verb%ts_rational::mulby% no longer
overrides the base class virtual! And we can't change the base class virtual
method either because that would break encapsulation and a lot of other
basic principles.

What's the solution? The answer is simple: abandon object orientation
altogether and lets try another paradigm.

\section{Functional Programming}
Functional programming is ideal for performing calculations.
One of the most basic principles is that objects are {\em immutable.}
Combined with the idea that functions should be {\em pure}, this
provides a powerful property known as {\em referential transparency}.

Let us recode our example using functional programming concepts:
\begin{minted}{c++}
  rational operator * (rational other) const { 
    int num = numerator * other.numerator;
    int den = denominator * other.denomintaor;
    int g = gcd(num,den);
    int n = num / g;
    int d = den / g;
    return rational(n,d);
  }
\end{minted}

This code is not yet good. Let us assume the immutable property for the
moment, noting that the \verb%const% qualifier on the method ensures
that the object cannot be modified. The problem we have here is that
the \verb%rational% constructor is invoked and although it does the right
thing, it breaks the rule that public methods should never be called
by any method. Consequently it does a useless check for zero,
fiddles the sign calculation and find the gcd and divides by it
pointlessly, because it is already done.

To fix this, we need another constructor:
\begin{minted}{c++}
private:
  rational(int n, int d, int dummy) : numerator(n), denominator(d) {}
\end{minted}

We added a useless dummy argument because C++ doesn't provide another
way to do this. In fact we are going to recode the public constructor
to use it as well:

\begin{minted}{c++}
class rational {
  int const numerator;
  int const denominator;
public:
   rational(int num, int den) {
     if(den == 0) throw "zero denominator";
     int sign = sgn(num) * sgn(den);
     unsigned int uden = abs(den);
     unsigned int unum = abs(num);
     unsigned int g = gcd(uden, unum);
     new(this) rational(
       sign * unum / g,
       denominator = uden / g,
       0
     );
   }
   ...
};
\end{minted}

This is a dirty technique! Languages have limitations and C++ many.
Luckily there is a workaround! The problem is we want to make the
non-static members \verb%const% to be sure they cannot be modified.
Unfortunately this prevents assignment. But there is a loophole
in the C++ type system, that inside a constructor the \verb%this%
pointer is always non-const, precisely to allow assignments 
in the body. In this case we still can't do assignments, but we can
use a placement new calling our private constructor to do the job.

It is not necessary to make the data members const, provided all
the methods do not modify these values: this can be assured by
making the methods const.

However making the data const is a great aid to reasoning because
it localises the assurance to two lines right at the top of the
class, rather than having to troll through all the methods checking
they're all const.

{\em Localistion} is a powerful tool used to make reasoning simpler.

Now you might say: well so what?

Well, our class is now thread safe!

Hey, what? Why? Well its clear, even if there is a race to get
at the two data members when copying a value or otherwise, it doesn't
matter because the variables cannot be changed! The only thing that
matters is that the values are both set before we access them,
and this is assured in normal good code because the constructor
is always executed in a single thread and completes before a binding
to the result is establshed. Of course, if you use say a placement
new to initialise a variable two threads have access to you 
{\em already} have a problem that the threads could look at the variable
before initialisation even starts. The point is, the functional code
cannot introduce any new problems,

\section{Optimisation}
Optimisation is a process whereby a human programmer of compiler
is able to improve the performance of a program by modifications
to the code which do not change the semantics.

One of the most common optimisations is to perform experiments on
a program which has several parameters which mediate tradeoffs
so find which combinations lead to the best performance for
particular kinds of data sets. This is known as {\em parametric tuning}.
It is very common to tune garbage collectors.

There are specialised tools to help identify performance issues
called {\em profilers}. They are generally easy to deply to check for
bottlenecks but quite difficult to use for tuning.

\section{Localisation}
{\em Localisation} is an important technique to improve the ability
to reason about code. For example consider the following

\begin{minted}{c++}
struct X {
  int y;
  void inc();
};
void X::inc(){ ++y; }
\end{minted} 

compared to

\begin{minted}{c++}
struct X {
  int y;
  void inc(){ ++y; }
};
\end{minted} 

The second case is shorter and the encoding of the \verb%inc% method 
involves incrementing the non-static member \verb%y% which is defined
on the line above. In the first case we have to look three lines up
to find \verb%y% is an \verb%int% and two lines down from the declaration
of \verb%inc% to find its definition. Worse, if the signature of
\verb%inc% changes we also have to change the definition. In addition,
the name of the struct, \verb%X% has to be repeated and so when seeing
the out of line definition we have to again look up to find the 
struct declaration.

Another example:

\begin{minted}{c++}
mutex lock;
void f() {
  lock_guard<mutex> locked(lock); 
  // do stuff holding mutex
}
\end{minted}

illustrates the Resource Acquisition Is Initialisation (RAII)
idea in C++. Using the guard not only acquires the lock, it
ensures the lock is held for the whole scope of the function 
\verb%f%, and what's more, it is correctly released when the
function exits, even if it does so as the result of an exception
being thrown. In this case a single line not only acquires
the lock, but also ensures its release. Compare this with:

\begin{minted}{c++}
mutex lock;
void f() {
  try {
    lock.lock();
    // do stuff holding mutex
    lock.unlock();
  }
  catch (...){ 
    lock.unlock(); throw; 
  }
}
\end{minted}

I hope this is equivalent but it could fail is the locking
the lock throws an exception! Can this happen? I have to look
up the C++ Standard to find out. TL;DR. The code is unreliable.
It is longer, it is clearly harder to decide if the code is correct,
and the code is fragile because we have to be careful modifying it.
What if we needed to lock two locks? What happens if the unlock
operation fails? Can that throw an exception? Does it matter?

The utility of localisation is not merely a coding technique
for it depends on the language design supporting it. In C++
the coupling of object destruction to release associated
resources with the release of the object memory in the
the \verb%delete% expression and the ability to define for
a class the destructor performing the relevant operations
inside the class and lexically close to constructor and 
data members holding the resources is a significant aid
to reasoning. It is certainly better than writing the release
code every time an object is freed!

Such is the strength of belief in localisation of this kind
that many languages have been designed to support this kind
of localisation, and in other cases, new versions of the language
added support later.

Here's an example from Java:
\begin{minted}{java}
class TestFinallyBlock {    
  public static void main(String args[]){    
    try{    }    
    catch(NullPointerException e){  }    
    finally {  }
  }
};
\end{minted}
The \verb%finally% clause is part of the \verb%try/catch/finally% statement,
and provides a local assurance some code will be executed whether
or not an exception is thrown, in other words {\em always.} Knowing
something always happens is easier to understand than something that
happens depending on a condition.

The same idea in golang:
\begin{minted}{go}
package main
import "fmt"
func main() {
	defer fmt.Println("world")
	fmt.Println("hello")
}
\end{minted}

The deferred operation is invoked at the end of its containing scope,
but it written at the start.

A related notion is that of a lexical closure. In Ocaml for example:

\begin{minted}{ocaml}
let f x = let g y = x + y in g
\end{minted}

the function \verb%g% is returned from \verb%f%, closed over
its environment, which contains the parameter \verb%x% of f,
as well as its own parameter \verb%y%. The point of a lexical
closure is that by specification it implies proximity of physical
code, that is, the actual characters of text: that's what lexical means.
So the idea of a lexical closure is you can reason about what the
function does only looking in a small space in the text file.

In summary, localisation is a way of enabling {\em local reasoning.}
We need to complete our survey with a counter example! Hindley Milner
type inference is our example .. and it explains why I hate it!
From Ocaml:

\begin{minted}{ocaml}
let f g = g 1 in
let h g = g 42.3 in
let k g1 g2 x = g1 x + g2 x in
let g i = k f h i in
g 1
\end{minted}

This gives an error but it is not clear unless you duplicate
the compilers reasoning. The function \verb%f% accepts a function which
accepts an integer, and \verb%h% accepts a function which accepts a float.
The function k takes two functions which accept the same type and return
an int. Now, when k is applied inside g it passes f and h which apply each
to the same value parameter x must be both a float and an int which is
impossible so we get an error in function \verb%k%. But actually we meant to write:

\begin{minted}{ocaml}
let f g = g 1.0 in
\end{minted}

as the first line. So the error message is entirely wrong.
the problem is the Ocaml compiler isn't smart enough to track
how it made the inferences and report all the possible sources
of the error, which is a good thing also since the inference
is accumulated by refining the set of possible types on ever
line of the program above.

A proper error report would resemble the mess you get with
template instantiation errors in C++: the compiler simply doesn't
have any way to determine where the real error is because it
is using global reasoning instead of local reasoning.

When Microsoft introduced type inference into C# they took great
care to ensure it only worked locally, precisely to prevent
the above problem.

It is instructive, in the Ocaml example, to learn how to find the
error: one adds type annotations for example:

\begin{minted}{ocaml}
let f (g: int -> int) = g 1 in
...
\end{minted}

and now we have localised the inference by forcing it to
agree with the parameter immediately, which immediately
finds the real error. The point again is that lexical
proximity can provide local reasoning.

There are many programmers like me, who do not actually read
code! It's true, if you give me code to examine I won't read it.
I just look at it wholistically. If it isn't indented properly
I'll throw it back at you or immediately edit it so it is.
I can't read code that doesn't have structure because I learn
what the code does from its visual structure.

Humans eyes do massively parallel processing and can see two
dimensional patterns almost instantaneously. Reading code
is a linear exercise which requires scanning laboriously
and remembering what you've already read. The best way to
understand code is to only read the bits you actually need to,
and the wholistic pattern matching is how to identify which
bits need further examination and thought.

Unsurprisingly compilers are better at local reasoning too!
In particular complex analyses such as data flow analysis
are almost universally done locally, within a single function
only, and then, the local results painfully coupled with some
ad hoc algorithms, since the analysis is an intractible $O(N^3)$
or worse. This is fine inside a small function, but we need to
find a way to combine the results which is $O(N log N)$.
This is the biggest number which is considered scalable.


\section{Anonymity}
\section{Indeterminacy}
\section{Abstraction}
\section{Reentrancy}

\section{Principles}
\subsubsection{Open/Closed Principle}
\subsubsection{Principle of Explicit Interfaces}
\subsubsection{SOLID}
The so-called SOLID principles of object orientation are attributed
to Robert Martin. These are:
\begin{Itemize}
\item[Single Responsibility Principle] states that a class should have only
one responsibility.
\item[Open/Closed Principle] we have already met and was first 
espoused by Bertrand Meyer in Object Oriented Software Construction.
The principle is a core principle of modularity and is not specific
to object oriented programming.
\item[Liskov Substitution Principle] is a behavioural specification
of the notion of subtyping. It is often paraphrased by saying
"if it looks like a duck and acts like a duck then it is a duck".
Unfortunately whilst the idea seems good the technical details
are suspect.
\item[Interface Segregation Principle] claims that a client shouldn't
for forced to implement an interface or method they do not need.
\tem[Dependency Inversion Principle] states that entities should depend
on abstractions not concretions.

\end{itemize}

\end{document}

